{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88119c3a-18dd-49bc-ba52-b7a7be13f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import functions # specific module for additional functions for this code\n",
    "from glob import glob\n",
    "pd.set_option('display.width', 2000)\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e91f2-6418-4691-a29b-d2dc99dd25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://sanctionslistservice.ofac.treas.gov/api/PublicationPreview/exports/SDN_ADVANCED.XML'\n",
    "\n",
    "response = requests.get(url)\n",
    "xml_content = response.content\n",
    "root = ET.fromstring(xml_content)\n",
    "\n",
    "# Namespace handling\n",
    "ns = {'ns': 'https://sanctionslistservice.ofac.treas.gov/api/PublicationPreview/exports/ADVANCED_XML'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f8fa1-8f96-458f-9eb2-cf3ce1b6dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from now on, there will be many cells similar to the following, as this is the structure of the OFAC XML file: \n",
    "# it is essentially an entire relational database, with many encoding tables all linked by cross-referenced keys to the most important tables that will be extracted later in this code\n",
    "# so I'll comment once for all cause I'm too lazy and it's really straightforward :)\n",
    "\n",
    "# find the 'AliasTypeValues' section\n",
    "alias_types = root.find('.//ns:AliasTypeValues', ns)\n",
    "\n",
    "# extract data into a list of dictionaries\n",
    "data_alias_types = []\n",
    "for alias_type in alias_types:\n",
    "    id = alias_type.attrib['ID']\n",
    "    text = alias_type.text\n",
    "    data_alias_types.append({'AliasTypeID': id, 'AliasType_text': text})\n",
    "\n",
    "# create the DataFrame\n",
    "df_alias_type = pd.DataFrame(data_alias_types)\n",
    "\n",
    "# %%\n",
    "idreg_types = root.find('.//ns:IDRegDocTypeValues', ns)\n",
    "\n",
    "data_reg_types = []\n",
    "for reg_type in idreg_types:\n",
    "    id = reg_type.attrib['ID']\n",
    "    text = reg_type.text\n",
    "    data_reg_types.append({'IDRegDocTypeID': id, 'IDRegDocType_text': text})\n",
    "\n",
    "df_reg_type = pd.DataFrame(data_reg_types)\n",
    "\n",
    "# %%\n",
    "idregdocuments_types = root.find('.//ns:IDRegDocuments', ns)\n",
    "\n",
    "data_regdocuments_types = []\n",
    "for r in idregdocuments_types:\n",
    "    id = r.attrib['ID']\n",
    "    reg_doc_id = r.attrib['IDRegDocTypeID']\n",
    "    identity_id = r.attrib['IdentityID']\n",
    "    id_registration_no = r.find('ns:IDRegistrationNo', ns).text if r.find('ns:IDRegistrationNo', ns) is not None else None\n",
    "    data_regdocuments_types.append({'IDRegDocument_id': id, 'IDRegDocTypeID': reg_doc_id, 'IdentityID_id': identity_id, 'IDRegistrationNo': id_registration_no})\n",
    "\n",
    "df_regdocuments_type = pd.DataFrame(data_regdocuments_types)\n",
    "\n",
    "# %%\n",
    "# that's actually the first example of the structure of the XML: \n",
    "# I'm merging the DataFrames df_regdocuments_type and df_reg_type on the key 'IDRegDocTypeID' to have a complete table\n",
    "id_reg_ofac = pd.merge(df_regdocuments_type, df_reg_type, on='IDRegDocTypeID', how='right')\n",
    "\n",
    "# %%\n",
    "area_codes = root.find('.//ns:AreaCodeValues', ns)\n",
    "\n",
    "data_area_code = []\n",
    "for area_code in area_codes:\n",
    "    id = area_code.attrib['ID']\n",
    "    country_id = area_code.attrib['CountryID']\n",
    "    description = area_code.attrib['Description']\n",
    "    text = area_code.text\n",
    "    data_area_code.append({'area_code_id': id, 'CountryID': country_id, 'description': description, 'iso2': text})\n",
    "\n",
    "df_area_code = pd.DataFrame(data_area_code)\n",
    "\n",
    "# %%\n",
    "detail_ref = root.find('.//ns:DetailReferenceValues', ns)\n",
    "\n",
    "data_detail_ref = []\n",
    "for d in detail_ref:\n",
    "    id = d.attrib['ID']\n",
    "    text = d.text\n",
    "    data_detail_ref.append({'DetailReferenceID': id, 'DetailReference_text': text})\n",
    "\n",
    "df_detail_ref = pd.DataFrame(data_detail_ref)\n",
    "\n",
    "# %%\n",
    "detail_type = root.find('.//ns:DetailTypeValues', ns)\n",
    "\n",
    "data_detail_type = []\n",
    "for d in detail_type:\n",
    "    id = d.attrib['ID']\n",
    "    text = d.text\n",
    "    data_detail_type.append({'DetailTypeID': id, 'DetailType_text': text})\n",
    "\n",
    "df_detail_type = pd.DataFrame(data_detail_type)\n",
    "\n",
    "# %%\n",
    "feature_type = root.find('.//ns:FeatureTypeValues', ns)\n",
    "\n",
    "data_feature_type = []\n",
    "for b in feature_type:\n",
    "    id = b.attrib['ID']\n",
    "    feature_type_group_id = b.attrib['FeatureTypeGroupID']\n",
    "    text = b.text\n",
    "    data_feature_type.append({'FeatureTypeID': id, 'FeatureTypeGroupID': feature_type_group_id, 'FeatureType_text': text})\n",
    "\n",
    "df_feature_type = pd.DataFrame(data_feature_type)\n",
    "\n",
    "# %%\n",
    "date_type = root.find('.//ns:IDRegDocDateTypeValues', ns)\n",
    "\n",
    "data_date_type = []\n",
    "for b in date_type:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_date_type.append({'date_type_id': id, 'date_type_text': text})\n",
    "\n",
    "df_date_type = pd.DataFrame(data_date_type)\n",
    "\n",
    "# %%\n",
    "legal_basis = root.find('.//ns:LegalBasisValues', ns)\n",
    "\n",
    "data_legal_basis = []\n",
    "for b in legal_basis:\n",
    "    id = b.attrib['ID']\n",
    "    legal_basis_short_ref = b.attrib['LegalBasisShortRef']\n",
    "    legal_basis_type_id = 'Unknown'\n",
    "    sanction_prg_id = b.attrib['SanctionsProgramID']\n",
    "    text = b.text\n",
    "    data_legal_basis.append({'LegalBasisID': id, 'LegalBasiShortRef': legal_basis_short_ref, 'LegalBasisTypeID': legal_basis_type_id, 'SanctionsProgramID': sanction_prg_id, 'LegalBasis_text': text})\n",
    "\n",
    "df_legal_basis = pd.DataFrame(data_legal_basis)\n",
    "\n",
    "# %%\n",
    "list_values = root.find('.//ns:ListValues', ns)\n",
    "\n",
    "data_list_values = []\n",
    "for b in list_values:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_list_values.append({'ListID': id, 'List_text': text})\n",
    "\n",
    "df_list_values = pd.DataFrame(data_list_values)\n",
    "\n",
    "# %%\n",
    "loc_part_type = root.find('.//ns:LocPartTypeValues', ns)\n",
    "\n",
    "data_loc_part_type = []\n",
    "for b in loc_part_type:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_loc_part_type.append({'LocPartTypeID': id, 'LocPartType_text': text})\n",
    "\n",
    "df_loc_part_type = pd.DataFrame(data_loc_part_type)\n",
    "\n",
    "# %%\n",
    "name_part_type = root.find('.//ns:NamePartTypeValues', ns)\n",
    "\n",
    "data_name_part_type = []\n",
    "for b in name_part_type:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_name_part_type.append({'NamePartTypeID': id, 'NamePartType_text': text})\n",
    "\n",
    "df_name_part_type = pd.DataFrame(data_name_part_type)\n",
    "\n",
    "# %%\n",
    "org_val = root.find('.//ns:OrganisationValues', ns)\n",
    "\n",
    "data_org_val = []\n",
    "for b in org_val:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_org_val.append({'org_id': id, 'org_text': text})\n",
    "\n",
    "df_org_val = pd.DataFrame(data_org_val)\n",
    "\n",
    "# %%\n",
    "party_sub_type = root.find('.//ns:PartySubTypeValues', ns)\n",
    "\n",
    "data_party_sub_type = []\n",
    "for b in party_sub_type:\n",
    "    id = b.attrib['ID']\n",
    "    party_type_id = b.attrib['PartyTypeID']\n",
    "    text = b.text\n",
    "    data_party_sub_type.append({'PartySubTypeID': id, 'PartyTypeID': party_type_id, 'PartySubType_text': text})\n",
    "\n",
    "df_party_sub_type = pd.DataFrame(data_party_sub_type)\n",
    "\n",
    "# %%\n",
    "party_type = root.find('.//ns:PartyTypeValues', ns)\n",
    "\n",
    "data_party_type = []\n",
    "for b in party_type:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_party_type.append({'PartyTypeID': id, 'PartyType_text': text})\n",
    "\n",
    "df_party_type = pd.DataFrame(data_party_type)\n",
    "\n",
    "# %%\n",
    "relation_qual = root.find('.//ns:RelationQualityValues', ns)\n",
    "\n",
    "data_relation_qual = []\n",
    "for b in relation_qual:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_relation_qual.append({'relation_quality_id': id, 'relation_quality_text': text})\n",
    "\n",
    "df_relation_qual = pd.DataFrame(data_relation_qual)\n",
    "\n",
    "# %%\n",
    "relation_type = root.find('.//ns:RelationTypeValues', ns)\n",
    "\n",
    "data_relation_type = []\n",
    "for b in relation_type:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_relation_type.append({'relation_type_id': id, 'relation_type_text': text})\n",
    "\n",
    "df_relation_type = pd.DataFrame(data_relation_type)\n",
    "\n",
    "# %%\n",
    "reliability = root.find('.//ns:ReliabilityValues', ns)\n",
    "\n",
    "data_reliability = []\n",
    "for b in reliability:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_reliability.append({'ReliabilityID': id, 'Reliability_text': text})\n",
    "\n",
    "df_reliability = pd.DataFrame(data_reliability)\n",
    "\n",
    "# %%\n",
    "sanction_prg = root.find('.//ns:SanctionsProgramValues', ns)\n",
    "\n",
    "data_sanction_prg = []\n",
    "for b in sanction_prg:\n",
    "    id = b.attrib['ID']\n",
    "    sub_body_id = b.attrib['SubsidiaryBodyID']\n",
    "    text = b.text\n",
    "    data_sanction_prg.append({'SanctionsProgramID': id, 'sub_body_id': sub_body_id, 'SanctionsProgram_text': text})\n",
    "\n",
    "df_sanction_prg = pd.DataFrame(data_sanction_prg)\n",
    "\n",
    "# %%\n",
    "sanction = root.find('.//ns:SanctionsTypeValues', ns)\n",
    "\n",
    "data_sanction = []\n",
    "for b in sanction:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_sanction.append({'SanctionsTypeID': id, 'SanctionsType_text': text})\n",
    "\n",
    "df_sanction_type = pd.DataFrame(data_sanction)\n",
    "\n",
    "# %%\n",
    "validity = root.find('.//ns:ValidityValues', ns)\n",
    "\n",
    "data_validity = []\n",
    "for b in validity:\n",
    "    id = b.attrib['ID']\n",
    "    text = b.text\n",
    "    data_validity.append({'validity_id': id, 'validity_text': text})\n",
    "\n",
    "df_validity = pd.DataFrame(data_validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf1459-a0bf-44b0-a083-596565c2f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Locations' is one of the tables I was talking about. It's much deeeper and nested, as you can see from teh code below\n",
    "locations_section = root.findall('.//ns:Locations/ns:Location', ns)\n",
    "\n",
    "data_locations = []\n",
    "\n",
    "for location in locations_section:\n",
    "    location_id = location.attrib.get('ID')\n",
    "\n",
    "    area_code_element = location.find('ns:LocationAreaCode', ns)\n",
    "    area_code_id = area_code_element.attrib.get('AreaCodeID') if area_code_element is not None else None\n",
    "\n",
    "    country_element = location.find('ns:LocationCountry', ns)\n",
    "    country_id = country_element.attrib.get('CountryID') if country_element is not None else None\n",
    "    country_relevance_id = country_element.attrib.get('CountryRelevanceID') if country_element is not None else None\n",
    "\n",
    "    loc_part_element = location.find('ns:LocationPart', ns)\n",
    "    loc_part_type_id = loc_part_element.attrib.get('LocPartTypeID') if loc_part_element is not None else None\n",
    "\n",
    "    loc_part_value_element = location.find('ns:LocationPart/ns:LocationPartValue', ns)\n",
    "    loc_part_value = loc_part_value_element.find('ns:Value', ns).text if loc_part_value_element is not None else None\n",
    "    primary = loc_part_value_element.attrib.get('Primary') if loc_part_value_element is not None else None\n",
    "    loc_part_value_type_id = loc_part_value_element.attrib.get('LocPartValueTypeID') if loc_part_value_element is not None else None\n",
    "    loc_part_value_status_id = loc_part_value_element.attrib.get('LocPartValueStatusID') if loc_part_value_element is not None else None\n",
    "\n",
    "    feature_version_element = location.find('ns:FeatureVersionReference', ns)\n",
    "    feature_version_id = feature_version_element.attrib.get('FeatureVersionID') if feature_version_element is not None else None\n",
    "\n",
    "    data_locations.append({\n",
    "        'LocationID': location_id,\n",
    "        'AreaCodeID': area_code_id,\n",
    "        'CountryID': country_id,\n",
    "        'CountryRelevanceID': country_relevance_id,\n",
    "        'LocPartTypeID': loc_part_type_id,\n",
    "        'LocPartValue': loc_part_value,\n",
    "        'Primary': primary,\n",
    "        'LocPartValueTypeID': loc_part_value_type_id,\n",
    "        'LocPartValueStatusID': loc_part_value_status_id,\n",
    "        'FeatureVersionID': feature_version_id\n",
    "    })\n",
    "\n",
    "df_locations = pd.DataFrame(data_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928ebd6-aa5b-4c8a-bca7-0401758b8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'DistinctParties' is the most important table, and the central one of the XML. It contains all the info about the sanctioned subjects themselves \n",
    "# it's really nested and if you need more data, you can check directly the orginal file\n",
    "parties_section = root.findall('.//ns:DistinctParty', ns)\n",
    "\n",
    "data_parties = []\n",
    "\n",
    "for party in parties_section:\n",
    "    fixed_ref = party.attrib.get('FixedRef')\n",
    "    comment = party.find('ns:Comment', ns).text if party.find('ns:Comment', ns) is not None else None\n",
    "    profile = party.find('ns:Profile', ns)\n",
    "\n",
    "    profile_id = profile.attrib.get('ID') if profile is not None else None\n",
    "    party_subtype_id = profile.attrib.get('PartySubTypeID') if profile is not None else None\n",
    "\n",
    "    # 'Identity' data extraction\n",
    "    identities_data = []\n",
    "    identities = profile.findall('ns:Identity', ns) if profile is not None else []\n",
    "    for identity in identities:\n",
    "        identity_id = identity.attrib.get('ID')\n",
    "        primary = identity.attrib.get('Primary')\n",
    "        false = identity.attrib.get('False')\n",
    "\n",
    "        # 'Alias' data extraction\n",
    "        aliases_data = []\n",
    "        aliases = identity.findall('ns:Alias', ns)\n",
    "        for alias in aliases:\n",
    "            alias_fixed_ref = alias.attrib.get('FixedRef')\n",
    "            alias_type_id = alias.attrib.get('AliasTypeID')\n",
    "            alias_primary = alias.attrib.get('Primary')\n",
    "            alias_low_quality = alias.attrib.get('LowQuality')\n",
    "\n",
    "            documented_name = alias.find('ns:DocumentedName', ns)\n",
    "            documented_name_id = documented_name.attrib.get('ID') if documented_name is not None else None\n",
    "            doc_name_status_id = documented_name.attrib.get('DocNameStatusID') if documented_name is not None else None\n",
    "\n",
    "            name_parts = documented_name.findall('ns:DocumentedNamePart/ns:NamePartValue', ns) if documented_name is not None else []\n",
    "            for name_part in name_parts:\n",
    "                name_part_value = name_part.text\n",
    "                name_part_group_id = name_part.attrib.get('NamePartGroupID')\n",
    "                script_id = name_part.attrib.get('ScriptID')\n",
    "                script_status_id = name_part.attrib.get('ScriptStatusID')\n",
    "                acronym = name_part.attrib.get('Acronym')\n",
    "\n",
    "                aliases_data.append({\n",
    "                    'AliasFixedRef': alias_fixed_ref,\n",
    "                    'AliasTypeID': alias_type_id,\n",
    "                    'AliasPrimary': alias_primary,\n",
    "                    'AliasLowQuality': alias_low_quality,\n",
    "                    'DocumentedNameID': documented_name_id,\n",
    "                    'DocNameStatusID': doc_name_status_id,\n",
    "                    'NamePartValue': name_part_value,\n",
    "                    'NamePartGroupID': name_part_group_id,\n",
    "                    'ScriptID': script_id,\n",
    "                    'ScriptStatusID': script_status_id,\n",
    "                    'Acronym': acronym\n",
    "                })\n",
    "\n",
    "        # 'NamePartGroups' data extraction\n",
    "        name_part_groups = identity.findall('ns:NamePartGroups/ns:MasterNamePartGroup/ns:NamePartGroup', ns)\n",
    "        name_part_groups_data = [{'NamePartGroupID': n.attrib.get('ID'), 'NamePartTypeID': n.attrib.get('NamePartTypeID')} for n in name_part_groups]\n",
    "\n",
    "        identities_data.append({\n",
    "            'IdentityID': identity_id,\n",
    "            'Primary': primary,\n",
    "            'False': false,\n",
    "            'Aliases': aliases_data,\n",
    "            'NamePartGroups': name_part_groups_data\n",
    "        })\n",
    "\n",
    "    # 'Feature' data extraction\n",
    "    features_data = []\n",
    "    features = profile.findall('ns:Feature', ns) if profile is not None else []\n",
    "    for feature in features:\n",
    "        feature_id = feature.attrib.get('ID')\n",
    "        feature_type_id = feature.attrib.get('FeatureTypeID')\n",
    "\n",
    "        feature_version = feature.find('ns:FeatureVersion', ns)\n",
    "        feature_version_id = feature_version.attrib.get('ID') if feature_version is not None else None\n",
    "        reliability_id = feature_version.attrib.get('ReliabilityID') if feature_version is not None else None\n",
    "\n",
    "        feature_comment = feature_version.find('ns:Comment', ns).text if feature_version.find('ns:Comment', ns) is not None else None\n",
    "\n",
    "        # 'DatePeriod' data extraction\n",
    "        date_period = feature_version.find('ns:DatePeriod', ns)\n",
    "        date_period_data = {}\n",
    "        if date_period is not None:\n",
    "            calendar_type_id = date_period.attrib.get('CalendarTypeID')\n",
    "            year_fixed = date_period.attrib.get('YearFixed')\n",
    "            month_fixed = date_period.attrib.get('MonthFixed')\n",
    "            day_fixed = date_period.attrib.get('DayFixed')\n",
    "\n",
    "            start = date_period.find('ns:Start', ns)\n",
    "            end = date_period.find('ns:End', ns)\n",
    "\n",
    "            start_from = start.find('ns:From', ns) if start is not None else None\n",
    "            start_to = start.find('ns:To', ns) if start is not None else None\n",
    "            end_from = end.find('ns:From', ns) if end is not None else None\n",
    "            end_to = end.find('ns:To', ns) if end is not None else None\n",
    "\n",
    "            start_from_date = (start_from.find('ns:Year', ns).text, start_from.find('ns:Month', ns).text, start_from.find('ns:Day', ns).text) if start_from is not None else (None, None, None)\n",
    "            start_to_date = (start_to.find('ns:Year', ns).text, start_to.find('ns:Month', ns).text, start_to.find('ns:Day', ns).text) if start_to is not None else (None, None, None)\n",
    "            end_from_date = (end_from.find('ns:Year', ns).text, end_from.find('ns:Month', ns).text, end_from.find('ns:Day', ns).text) if end_from is not None else (None, None, None)\n",
    "            end_to_date = (end_to.find('ns:Year', ns).text, end_to.find('ns:Month', ns).text, end_to.find('ns:Day', ns).text) if end_to is not None else (None, None, None)\n",
    "\n",
    "            date_period_data = {\n",
    "                'CalendarTypeID': calendar_type_id,\n",
    "                'YearFixed': year_fixed,\n",
    "                'MonthFixed': month_fixed,\n",
    "                'DayFixed': day_fixed,\n",
    "                'StartFromDate': start_from_date,\n",
    "                'StartToDate': start_to_date,\n",
    "                'EndFromDate': end_from_date,\n",
    "                'EndToDate': end_to_date\n",
    "            }\n",
    "\n",
    "        version_detail = feature_version.find('ns:VersionDetail', ns)\n",
    "        version_detail_type_id = version_detail.attrib.get('DetailTypeID') if version_detail is not None else None\n",
    "        version_detail_ref_id = version_detail.attrib.get('DetailReferenceID') if version_detail is not None else None\n",
    "\n",
    "        identity_reference = feature.find('ns:IdentityReference', ns)\n",
    "        identity_feature_link_type_id = identity_reference.attrib.get('IdentityFeatureLinkTypeID') if identity_reference is not None else None\n",
    "\n",
    "        version_location = feature_version.find('ns:VersionLocation', ns)\n",
    "        version_location_id = version_location.attrib.get('LocationID') if version_location is not None else None\n",
    "\n",
    "        features_data.append({\n",
    "            'FeatureID': feature_id,\n",
    "            'FeatureTypeID': feature_type_id,\n",
    "            'FeatureVersionID': feature_version_id,\n",
    "            'ReliabilityID': reliability_id,\n",
    "            'Comment': feature_comment,\n",
    "            'DatePeriod': date_period_data,\n",
    "            'VersionDetailTypeID': version_detail_type_id,\n",
    "            'VersionDetailReferenceID': version_detail_ref_id,\n",
    "            'IdentityFeatureLinkTypeID': identity_feature_link_type_id,\n",
    "            'VersionLocationID': version_location_id\n",
    "        })\n",
    "\n",
    "    data_parties.append({\n",
    "        'FixedRef': fixed_ref,\n",
    "        'Comment': comment,\n",
    "        'ProfileID': profile_id,\n",
    "        'PartySubTypeID': party_subtype_id,\n",
    "        'Identities': identities_data,\n",
    "        'Features': features_data\n",
    "    })\n",
    "\n",
    "df_parties = pd.json_normalize(data_parties, sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbb45b-2f0b-43cd-883f-28c233120e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'explode_nested_col' is a function called from the module defined below. For details about the function, please open the module and read the comments\n",
    "df_parties_exploded = functions.explode_nested_col(df_parties)\n",
    "df_parties_exploded_all = functions.explode_nested_col(df_parties_exploded)\n",
    "# df_parties_all = df_parties_tutto\n",
    "df_parties_all = df_parties_exploded_all.rename(columns={'Identities_Aliases_AliasTypeID': 'AliasTypeID', 'Features_VersionLocationID': 'LocationID', 'Features_DatePeriod.CalendarTypeID': 'CalendarTypeID',\n",
    "                                                         'Features_VersionDetailReferenceID': 'DetailReferenceID', 'Features_FeatureTypeID' : 'FeatureTypeID', 'Features_VersionDetailTypeID': 'DetailTypeID',\n",
    "                                                         'Identities_NamePartGroups_NamePartTypeID': 'NamePartTypeID', 'Features_ReliabilityID': 'ReliabilityID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f27c2f-f346-4d5e-b7b0-34ead6a86792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'SanctionsEntry' is one the most interesting tables. It contains all the info about the saanction itself (programme, regulation and so on) \n",
    "data = []\n",
    "\n",
    "for sanctions_entry in root.findall('.//ns:SanctionsEntry', ns):\n",
    "    base_data = {\n",
    "        'SanctionsEntry_ID': sanctions_entry.attrib.get('ID'),\n",
    "        'Profile_ID': sanctions_entry.attrib.get('ProfileID'),\n",
    "        'List_ID': sanctions_entry.attrib.get('ListID')\n",
    "    }\n",
    "    \n",
    "    # estraction of each 'EntryEvent' for each 'SanctionsEntry'\n",
    "    for entry_event in sanctions_entry.findall('ns:EntryEvent', ns):\n",
    "        event_data = {\n",
    "            'EntryEvent_ID': entry_event.attrib.get('ID'),\n",
    "            'EntryEventType_ID': entry_event.attrib.get('EntryEventTypeID'),\n",
    "            'LegalBasis_ID': entry_event.attrib.get('LegalBasisID')\n",
    "        }\n",
    "\n",
    "        # dealing with 'Comment' and 'Date' \n",
    "        comment = entry_event.find('ns:Comment', ns)\n",
    "        if comment is not None:\n",
    "            event_data['Event_Comment'] = comment.text\n",
    "\n",
    "        date = entry_event.find('ns:Date', ns)\n",
    "        if date is not None:\n",
    "            event_data['Event_Date_Year'] = date.find('ns:Year', ns).text\n",
    "            event_data['Event_Date_Month'] = date.find('ns:Month', ns).text\n",
    "            event_data['Event_Date_Day'] = date.find('ns:Day', ns).text\n",
    "\n",
    "        # estraction of each 'SanctionsMeasure' for each 'EntryEvent'\n",
    "        for sanctions_measure in sanctions_entry.findall('ns:SanctionsMeasure', ns):\n",
    "            measure_data = {\n",
    "                'SanctionsMeasure_ID': sanctions_measure.attrib.get('ID'),\n",
    "                'SanctionsType_ID': sanctions_measure.attrib.get('SanctionsTypeID')\n",
    "            }\n",
    "\n",
    "            # dealing with 'Comment' and 'DatePeriod'\n",
    "            comment = sanctions_measure.find('ns:Comment', ns)\n",
    "            if comment is not None:\n",
    "                measure_data['Measure_Comment'] = comment.text\n",
    "\n",
    "            date_period = sanctions_measure.find('ns:DatePeriod', ns)\n",
    "            if date_period is not None:\n",
    "                date = date_period.find('ns:Date', ns)\n",
    "                if date is not None:\n",
    "                    measure_data['Measure_Date_Year'] = date.find('ns:Year', ns).text\n",
    "                    measure_data['Measure_Date_Month'] = date.find('ns:Month', ns).text\n",
    "                    measure_data['Measure_Date_Day'] = date.find('ns:Day', ns).text\n",
    "\n",
    "            # combine 'base_data', 'event_data' and 'measure_data' in a new unique field\n",
    "            combined_data = {**base_data, **event_data, **measure_data}\n",
    "            data.append(combined_data)\n",
    "\n",
    "df_sanctions_entries = pd.DataFrame(data)\n",
    "df_sanctions_entries = df_sanctions_entries.rename(columns={'EntryEventType_ID': 'EventTypeID', 'LegalBasis_ID': 'LegalBasisID', 'List_ID': 'ListID',\n",
    "                                                            'SanctionsType_ID': 'SanctionsTypeID', 'Profile_ID': 'ProfileID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076d0d6-079a-4504-a14f-6aaf7b3c8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's time to aggregate all the tables extracted so far in a unique DataFrame\n",
    "# let's start with the smallest DataFrames\n",
    "merged_df_party_type = pd.merge(df_party_sub_type, df_party_type, on='PartyTypeID', how='left')\n",
    "merged_df_sanction_prg = pd.merge(df_sanction_prg, df_legal_basis, on='SanctionsProgramID', how='left')\n",
    "\n",
    "# 'locations' and 'sanctions' dfs\n",
    "merged_df_area_code = pd.merge(df_locations, df_area_code, on='CountryID', how='left')\n",
    "merged_df_loc_part_type = pd.merge(merged_df_area_code, df_loc_part_type, on='LocPartTypeID', how='left')\n",
    "merged_df_legal_basis = pd.merge(df_sanctions_entries, df_legal_basis, on='LegalBasisID', how='left')\n",
    "merged_df_list_values = pd.merge(merged_df_legal_basis, df_list_values, on='ListID', how='left')\n",
    "merged_df_sanctions = pd.merge(merged_df_list_values, df_sanction_type, on='SanctionsTypeID', how='left')\n",
    "\n",
    "# now the final aggregation\n",
    "merged_df_alias_type = pd.merge(df_parties_all, df_alias_type, on='AliasTypeID', how='left')\n",
    "merged_df_party_sub_type = pd.merge(merged_df_alias_type, merged_df_party_type, on='PartySubTypeID', how='left')\n",
    "merged_df_detail_ref = pd.merge(merged_df_party_sub_type, df_detail_ref, on='DetailReferenceID', how='left')\n",
    "merged_df_detail_type = pd.merge(merged_df_detail_ref, df_detail_type, on='DetailTypeID', how='left')\n",
    "merged_df_feature_type = pd.merge(merged_df_detail_type, df_feature_type, on='FeatureTypeID', how='left')\n",
    "merged_df_name_part_type = pd.merge(merged_df_feature_type, df_name_part_type, on='NamePartTypeID', how='left')\n",
    "merged_df_reliability = pd.merge(merged_df_name_part_type, df_reliability, on='ReliabilityID', how='left')\n",
    "merged_df_locations_final = pd.merge(merged_df_reliability, merged_df_loc_part_type, on='LocationID', how='left')\n",
    "merged_df_sanctions_final = pd.merge(merged_df_locations_final, merged_df_sanctions, on='ProfileID', how='left')\n",
    "\n",
    "merged_df_sanctions_final = pd.merge(merged_df_sanctions_final, id_reg_ofac, left_on='Identities_IdentityID', right_on='IdentityID_id', how='left')\n",
    "\n",
    "# I'm getting rid of useless dfs to optimize memory usage \n",
    "# I'm using Google cola, so in this environment there is no real need for that, I'm just including a section in which you can add all the delete you need\n",
    "del merged_df_sanctions\n",
    "del id_reg_ofac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbf2d5-b0fb-4d02-bc79-7b6d28f1b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I have to build one col for the listing date, because OFAC split it in three different columns\n",
    "merged_df_sanctions_final['listing_date'] = merged_df_sanctions_final['Event_Date_Day']+'/'+merged_df_sanctions_final['Event_Date_Month']+'/'+merged_df_sanctions_final['Event_Date_Year']\n",
    "merged_df_sanctions_final['listing_date'] = pd.to_datetime(merged_df_sanctions_final['listing_date'], format='%d/%m/%Y')\n",
    "\n",
    "# I create a dictinary for the columns needed. For sake of completeness, I leave here the list of columns name of merged_df_sanctions_final\n",
    "col_dict = {\n",
    "        'ProfileID': None,\n",
    "        #'Features_FeatureID': None,\n",
    "        #'DetailTypeID': None,\n",
    "        #'DetailReferenceID': None,\n",
    "        #'LocationID': None,\n",
    "        #'Features_DatePeriod.StartFromDate': None,\n",
    "        #'Features_DatePeriod.StartToDate': None,\n",
    "        #'Features_DatePeriod.EndFromDate': None,\n",
    "        #'Features_DatePeriod.EndToDate': None,\n",
    "        'Identities_Aliases_NamePartValue': None,\n",
    "        #'NamePartTypeID': None,\n",
    "        #'AliasType_text': None,\n",
    "        #'PartyTypeID': None,\n",
    "        #'PartySubType_text': None,\n",
    "        'PartyType_text': None,\n",
    "        #'CalendarType_text': None,\n",
    "        #'DetailReference_text': None,\n",
    "        #'DetailType_text': None,\n",
    "        #'FeatureTypeGroupID': None,\n",
    "        'FeatureType_text': None,\n",
    "        'NamePartType_text': None,\n",
    "        #'Reliability_text': None,\n",
    "        #'AreaCodeID': None,\n",
    "        #'CountryID': None,\n",
    "        #'CountryRelevanceID': None,\n",
    "        #'LocPartTypeID': None,\n",
    "        'LocPartValue': None,\n",
    "        #'LocPartValueTypeID': None,\n",
    "        #'LocPartValueStatusID': None,\n",
    "        #'FeatureVersionID': None,\n",
    "        #'area_code_id': None,\n",
    "        'description': None,\n",
    "        'iso2': None,\n",
    "        #'CountryRelevanceID_text': None,\n",
    "        'LocPartType_text': None,\n",
    "        #'LocPartValueStatus_text': None,\n",
    "        #'SanctionsEntry_ID': None,\n",
    "        #'ListID': None,\n",
    "        #'EntryEvent_ID': None,\n",
    "        #'EventTypeID': None,\n",
    "        #'LegalBasisID': None,\n",
    "        #'Event_Comment': None,\n",
    "        #'Event_Date_Year': None,\n",
    "        #'Event_Date_Month': None,\n",
    "        #'Event_Date_Day': None,\n",
    "        'listing_date': None,\n",
    "        'SanctionsTypeID': None,\n",
    "        'Measure_Comment': None,\n",
    "        #'EventType_text': None,\n",
    "        'LegalBasiShortRef': None,\n",
    "        #'LegalBasisTypeID': None,\n",
    "        #'SanctionsProgramID': None,\n",
    "        'LegalBasis_text': None,\n",
    "        'List_text': None,\n",
    "        'SanctionsType_text': None,\n",
    "        'IDRegistrationNo': None,\n",
    "        'IDRegDocType_text': None\n",
    "}\n",
    "\n",
    "selected_col = list(col_dict.keys())\n",
    "\n",
    "df_ofac_analysis = merged_df_sanctions_final[selected_col]\n",
    "df_ofac_analysis = df_ofac_analysis.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5e89c-20de-492a-8ab4-d42354239998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of needed columns\n",
    "df_ofac_analysis = df_ofac_analysis.rename(columns={'Identities_Aliases_NamePartValue': 'sanctioned_alias'})\n",
    "df_ofac_analysis = df_ofac_analysis[['ProfileID', 'iso2', 'PartyType_text', 'listing_date', 'LegalBasis_text']]\n",
    "df_ofac_analysis['sanction_body'] = 'OFAC'\n",
    "\n",
    "# standardize columns name: \n",
    "# everytihng that's referred to the single sanctioned subject has 'sanctioned_' before the col name\n",
    "# everytihng that's referred to the sanction itself has 'sanction_' before the col name\n",
    "df_ofac_analysis = df_ofac_analysis.rename(columns={'ProfileID': 'sanctioned_id',\n",
    "                                                    'iso2': 'sanctioned_country_iso2',\n",
    "                                                    'PartyType_text': 'sanctioned_type',\n",
    "                                                    'listing_date': 'sanction_listing_date',\n",
    "                                                    'LegalBasis_text': 'sanction_text'})\n",
    "df_ofac_analysis = df_ofac_analysis.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f28839-d8b6-485a-81e4-262a90147414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell saves the DataFrame on your google drive (I use Google colab). You can just save he DataFrame on your local directory as well\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df_ofac_analysis.to_csv('/content/drive/My Drive/df_ofac_analysis.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

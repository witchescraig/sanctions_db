{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c0b95-624e-4a6a-a782-51e50973cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import functions # specific module for additional functions for this code\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463775a-9c41-42e3-b061-085782408a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to read the html file, I do prefer it with respect to xml\n",
    "url = 'https://docs.fcdo.gov.uk/docs/UK-Sanctions-List.html'\n",
    "response = requests.get(url)\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf10148-6fba-4ed4-84eb-bbf566e52a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the whole html text in a string list by entity id \n",
    "string_html = html.replace('\\n', ' ').strip()\n",
    "pattern = r'(Unique ID)'\n",
    "string_w_separator = re.sub(pattern, r'## \\1', string_html)\n",
    "list_html = string_w_separator.split('##')\n",
    "# filter blank spaces\n",
    "list_html = [x.strip() for x in list_html if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48778593-a0fe-4946-a187-75411838e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i loop to extract the info from each html entity id cube\n",
    "# each of them has three sections: \n",
    "# 1. basic info on sanctioned subject\n",
    "# 2. names and aliases of sanctioned subject\n",
    "# 3. addresses and countries of sanctioned subject\n",
    "\n",
    "# I know that the first occurrence of list_html is useless because it contains only file generartion info\n",
    "r = len(list_html)-1\n",
    "\n",
    "# I define a counter, so as to know the status of advancement of the code\n",
    "counter = 0\n",
    "\n",
    "dfs_basic_info = []\n",
    "dfs_names = []\n",
    "dfs_addresses = []\n",
    "for i in range(1,len(list_html)):\n",
    "    \n",
    "    soup = BeautifulSoup(list_html[i], 'lxml')\n",
    "    \n",
    "    # DataFrame for basic info\n",
    "    unique_id = soup.find('span')\n",
    "    # the counter advances on each loop\n",
    "    counter += 1          \n",
    "    counter_message = f\"{unique_id.text.strip()} - status {counter}/{r}\"\n",
    "    print(counter_message, end='\\r') \n",
    "    \n",
    "    subject_type = unique_id.find_next('span').text.strip()\n",
    "    regime_name = soup.find('b', string=re.compile('Regime Name:', re.IGNORECASE)).find_next('span').text.strip()\n",
    "    try:\n",
    "        sanction_imposed = soup.find('b', string=re.compile('Sanctions Imposed:', re.IGNORECASE)).find_next('span').text.strip()\n",
    "    except:\n",
    "        sanction_imposed = None\n",
    "    \n",
    "    designation_source = soup.find('b', string=re.compile('Designation Source:', re.IGNORECASE)).find_next('span').text.strip()\n",
    "    date_designed = soup.find('b', string=re.compile('Date Designated:', re.IGNORECASE)).find_next('span').text.strip()\n",
    "    try:\n",
    "        ofsi_grp = soup.find('b', string=re.compile('OFSI Group ID:', re.IGNORECASE)).find_next('span').text.strip()\n",
    "    except:\n",
    "        ofsi_grp = None\n",
    "    try:\n",
    "        un_ref = soup.find('b', string=re.compile('UN Reference Number:', re.IGNORECASE)).find_next('span').text.strip()\n",
    "    except:\n",
    "        un_ref = None\n",
    "        \n",
    "    data_basic_info = {\n",
    "                    'sanctioned_id': unique_id.text.strip(),\n",
    "                    'sanctioned_type': subject_type,\n",
    "                    'sanction_listing_date': date_designed,\n",
    "                    'sanction_text': regime_name,\n",
    "                    'sanction_imposed': sanction_imposed,\n",
    "                    'sanctioned_ofsi_grp': ofsi_grp,\n",
    "                    'sanctioned_un_ref': un_ref,\n",
    "                    'sanction_body': 'UK'\n",
    "    }\n",
    "\n",
    "    # DataFrame for all aliases\n",
    "    df_basic_info = pd.DataFrame(data_basic_info, index=[0])\n",
    "    dfs_basic_info.append(df_basic_info)\n",
    "    names = soup.find_all('b', text=' Name: ')\n",
    "    for n in names:\n",
    "        try:\n",
    "            name_text = n.find_next('span').get_text(strip=True)\n",
    "        except:\n",
    "            name_text = None\n",
    "        try:\n",
    "            name_type = n.find_next('b', text = ' Name Type: ').find_next('span').get_text(strip=True)\n",
    "        except:\n",
    "            name_type = None\n",
    "        \n",
    "        sanctioned_id = unique_id.get_text(strip=True)\n",
    "        dfs_names.append({'sanctioned_id': sanctioned_id, 'sanctioned_alias': name_text, 'sanctioned_alias_type': name_type}) \n",
    "    \n",
    "    # DataFrame for all addresses\n",
    "    addresses = soup.find_all('b', text=' Address: ')\n",
    "    for a in addresses:\n",
    "        try:\n",
    "            add_txt = a.find_next('span').get_text(strip=True)\n",
    "        except:\n",
    "            add_txt = None\n",
    "        try:\n",
    "            add_country = a.find_next('b', text = ' Address Country: ').find_next('span').get_text(strip=True)\n",
    "        except:\n",
    "            add_country = None\n",
    "        \n",
    "        sanctioned_id = unique_id.get_text(strip=True)\n",
    "        dfs_addresses.append({'sanctioned_id': sanctioned_id, 'sanctioned_address': add_txt, 'sanctioned_country': add_country}) \n",
    "        \n",
    "        \n",
    "df_basic_info = pd.concat(dfs_basic_info, ignore_index=True)\n",
    "df_names = pd.DataFrame(dfs_names)\n",
    "df_addresses = pd.DataFrame(dfs_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda8e11-5fe1-4121-985f-12a60b674033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little bit of data amnipulation\n",
    "df_basic_info['sanctioned_type'] = df_basic_info['sanctioned_type'].str.replace('- ', '')\n",
    "df_basic_info['sanction_listing_date'] = pd.to_datetime(df_basic_info['sanction_listing_date'], format='%d/%m/%Y')\n",
    "\n",
    "# I only need to merge the addresses DataFrame, because it contains the 'sanctioned_country'\n",
    "df_final = pd.merge(df_basic_info, df_addresses, on='sanctioned_id', how='outer')\n",
    "\n",
    "df_uk_analysis = df_final[['sanctioned_id', 'sanctioned_country', 'sanctioned_type', 'sanction_text', 'sanction_listing_date', 'sanction_body']]\n",
    "df_uk_analysis = df_uk_analysis.drop_duplicates()\n",
    "df_uk_analysis[\"sanctioned_country_iso3\"] = df_uk_analysis[\"sanctioned_country\"].apply(functions.descr_to_iso3)\n",
    "\n",
    "# I had to add some adjustemts beacuse the file has invalid names for the lookup function \n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('Russia', 'RUS')\n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('Congo (Democratic Republic)', 'COD')\n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('DPRK', 'PRK')\n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('Kosovo', 'XKX')\n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('Occupied Palestinian Territories', 'PSE')\n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('The Gambia', 'GMB')\n",
    "df_uk_analysis['sanctioned_country_iso3'] = df_uk_analysis['sanctioned_country_iso3'].str.replace('Turkey', 'TUR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5ec98-30cc-4a90-ba52-0f8bf02446e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell saves the DataFrame on your google drive (I use Google colab). You can just save he DataFrame on your local directory as well\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df_uk_analysis.to_csv('/content/drive/My Drive/df_uk_analysis.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece10208-aecd-45a5-9a8b-270bb780c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_analysis.to_csv('C:/Users/valer/OneDrive/Desktop/python/input/df_uk_analysis.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
